# minGPT

![mingpt](mingpt.jpg)

PyTorch로 구현된 OpenAI GPT(특히 GPT-2) 모델의 미니멀 버전입니다. 학습(training) 및 추론(inference) 모두를 지원하며, 코드가 작고 직관적이어서 Transformer 기반 언어모델을 이해하고 실험하는 데 적합합니다.

> **참고:** 본 프로젝트는 반-아카이브(semi-archived) 상태입니다. 더 발전된 버전은 [nanoGPT](https://github.com/karpathy/nanoGPT)에서 확인할 수 있습니다.

---

## 구성 파일 및 예제

- `mingpt/model.py`: Transformer 모델 정의
- `mingpt/bpe.py`: Byte Pair Encoding 구현
- `mingpt/trainer.py`: 학습 루프 및 Trainer 클래스
- `projects/adder`: 숫자 덧셈 문제를 학습하는 GPT 예제
- `projects/chargpt`: 문자 단위 언어 모델 학습 예제
- `demo.ipynb`: 간단한 정렬 문제 예제 (노트북)
- `generate.ipynb`: 미리 학습된 GPT2로 텍스트 생성 (노트북)

---

## 설치 방법

```bash
git clone https://github.com/karpathy/minGPT.git
cd minGPT
pip install -e .
```
## 사용 예시

### 모델 생성 예시

```python
from mingpt.model import GPT
model_config = GPT.get_default_config()
model_config.model_type = 'gpt2'
model_config.vocab_size = 50257
model_config.block_size = 1024
model = GPT(model_config)
```

### 학습 예시

```python
# torch.utils.data.Dataset을 상속한 데이터셋 필요
train_dataset = YourDataset()

from mingpt.trainer import Trainer
train_config = Trainer.get_default_config()
train_config.learning_rate = 5e-4
train_config.max_iters = 1000
train_config.batch_size = 32
trainer = Trainer(train_config, model, train_dataset)
trainer.run()
```
더 자세한 예시는 demo.ipynb를 참고하세요.

## 유닛 테스트

```bash
python -m unittest discover tests
```

## TODO
임의 텍스트 파일로 GPT-2 파인튜닝 데모 추가

대화형 에이전트 데모 추가

각 프로젝트(Adder, Chargpt) 결과 문서화

혼합 정밀도 및 분산 학습 지원

벤치마크 재현(text8 등)

print 대신 로깅 적용

requirements.txt 파일 추가

다양한 사전학습 모델 가중치 지원

## 참고자료
[openai/gpt-2](https://github.com/openai/gpt-2)

[huggingface/transformers](https://github.com/huggingface/transformers)

GPT-1, GPT-2, GPT-3, Image GPT 논문 및 구현 노트
